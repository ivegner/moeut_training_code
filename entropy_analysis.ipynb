{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6500053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import transformer_lens\n",
    "from transformer_lens.utils import composition_scores\n",
    "from transformer_lens import FactoredMatrix, HookedTransformer\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "\n",
    "import einops as e\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_built() else \"cpu\"\n",
    "\n",
    "def make_even(u, s, v):\n",
    "    return FactoredMatrix(\n",
    "        u * s.sqrt()[..., None, :],\n",
    "        s.sqrt()[..., :, None] * transformer_lens.utils.transpose(v),\n",
    "    )\n",
    "\n",
    "def re_get_single_component(u, s, v, i):\n",
    "    news = s.clone()\n",
    "    newu = u  # .clone()\n",
    "    newv = v  # .clone()\n",
    "    news[:i] = 0\n",
    "    # newu[:, :i] = 0\n",
    "    # newv[:, :i] = 0\n",
    "    if i != len(s) - 1:\n",
    "        news[i + 1 :] = 0\n",
    "        # newu[:, i+1:] = 0\n",
    "        # newv[:, i+1:] = 0\n",
    "    return make_even(newu, news, newv)\n",
    "\n",
    "\n",
    "# create a heatmap of all heads in all layers\n",
    "def very_exhaustive_heatmap(layer_weights, is_moeut=True, all_to_all=True):\n",
    "    if is_moeut:\n",
    "        n_experts = layer_weights[0][\"n_experts\"][\"v\"]\n",
    "        assert (\n",
    "            n_experts == layer_weights[0][\"n_experts\"][\"o\"]\n",
    "        ), \"Expected n_experts to be the same for v and o\"\n",
    "    else:\n",
    "        n_experts = 1\n",
    "    n_heads = layer_weights[0][\"n_heads\"]\n",
    "    d_head = layer_weights[0][\"d_head\"]\n",
    "\n",
    "    heatmap = np.zeros(\n",
    "        (len(layer_weights), len(layer_weights), n_heads * n_experts, n_heads, d_head)\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for i, layer_from in enumerate(layer_weights):\n",
    "            for j, layer_to in enumerate(layer_weights):\n",
    "                if not all_to_all and i >= j:\n",
    "                    # if not universal transformer, only compute for\n",
    "                    # layers after layer_from\n",
    "                    continue\n",
    "                # compute composition scores from ov in layer i to qk in layer j\n",
    "                for ov_idx, ov in enumerate(layer_from[\"ov\"]):\n",
    "                    # ov = FactoredMatrix(ov.A.to(\"cuda\"), ov.B.to(\"cuda\"))\n",
    "                    src = ov.svd()\n",
    "                    for qk_idx, qk in enumerate(layer_to[\"qk\"]):\n",
    "                        print(f\"Computing heatmap for {i}.{ov_idx} -> {j}.{qk_idx}\")\n",
    "                        # qk = FactoredMatrix(qk.A.to(\"cuda\"), qk.B.to(\"cuda\"))\n",
    "                        # ov.A, ov.B = ov.A.to(\"cuda\"), ov.B.to(\"cuda\")\n",
    "                        # qk.A, qk.B = qk.A.to(\"cuda\"), qk.B.to(\"cuda\")\n",
    "                        # right = qk.svd()\n",
    "                        for k in range(layer_from[\"d_head\"]):\n",
    "                            src_comp = re_get_single_component(*src, k)\n",
    "                            s = composition_scores(src_comp, qk).item()\n",
    "                            heatmap[i, j, ov_idx, qk_idx, k] = s\n",
    "                            # src_comp.svd.cache_clear()\n",
    "                            del src_comp, s\n",
    "                        # qk.svd.cache_clear()\n",
    "                    # ov.svd.cache_clear()\n",
    "                    del src\n",
    "                torch.cuda.empty_cache()\n",
    "                FactoredMatrix.svd.cache_clear()\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "def get_weights_and_heatmap_from_path(layer_weights_path, all_to_all=False):\n",
    "    weights = torch.load(layer_weights_path, weights_only=False, map_location=device)\n",
    "    # heatmap is saved next to the layer_weights\n",
    "    heatmap_path = Path(layer_weights_path).with_name(\"heatmap.pkl\")\n",
    "    if not heatmap_path.exists():\n",
    "        heatmap = very_exhaustive_heatmap(\n",
    "            weights, is_moeut=\"moeut\" in layer_weights_path, all_to_all=all_to_all\n",
    "        )\n",
    "        with open(heatmap_path, \"wb\") as f:\n",
    "            torch.save(heatmap, f)\n",
    "    else:\n",
    "        heatmap = torch.load(heatmap_path, weights_only=False)\n",
    "\n",
    "    return weights, heatmap\n",
    "\n",
    "# Find average composition scores for random matrices of the same size\n",
    "def random_composition_heatmap(d_embed, d_head, n_layers=1, n_ov=10, n_qk=10):\n",
    "    assert n_ov==n_qk, \"For simplicity, require n_ov == n_qk\"\n",
    "    # Generate random matrices\n",
    "    def make_random_matrix():\n",
    "        return FactoredMatrix(\n",
    "            torch.randn(d_embed, d_head, device=device), torch.randn(d_head, d_embed, device=device)\n",
    "        )\n",
    "\n",
    "    layer_weights = [\n",
    "        {\n",
    "            \"ov\": [make_random_matrix() for _ in range(n_ov)],\n",
    "            \"qk\": [make_random_matrix() for _ in range(n_qk)],\n",
    "            \"d_head\": d_head,\n",
    "            \"n_heads\": n_ov,\n",
    "            \"n_experts\": {\"v\": 1, \"o\": 1},\n",
    "        } for _ in range(n_layers)\n",
    "    ]\n",
    "    # Compute composition scores\n",
    "    heatmap = very_exhaustive_heatmap(\n",
    "        layer_weights, is_moeut=True, all_to_all=True\n",
    "    )  # (n_layers, n_layers, n_ov, n_qk, d_head)\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "def compute_entropy(a, axis=-1):\n",
    "    a = np.array(a) / (np.sum(a, axis=axis, keepdims=True) + 1e-10)\n",
    "\n",
    "    return -np.sum(a * np.log(a + 1e-10), axis=axis)\n",
    "\n",
    "def plot_nd_heatmap_grid(\n",
    "    heatmap: np.ndarray,\n",
    "    all_to_all: bool = True,\n",
    "    cmin=None,\n",
    "    cmax=None,\n",
    "    layout_dict: dict = None,\n",
    "    subplot_title_func=None,\n",
    "    showtext=False,\n",
    "    heatmap_x_label=\"OV Head\",\n",
    "    heatmap_y_label=\"QK Head\",\n",
    "    heatmap_z_label=\"Value\",\n",
    "):\n",
    "    # if heatmap 4d, make subplots\n",
    "    # otherwise, make one subplot\n",
    "    if layout_dict is None:\n",
    "        layout_dict = dict()\n",
    "\n",
    "    if heatmap.ndim == 4:\n",
    "        IS_4D = True\n",
    "        if subplot_title_func is None:\n",
    "            subplot_title_func = lambda h, i, j: f\"{i}.OV -> {j}.QK\"  # noqa: E731\n",
    "\n",
    "        n_layers = heatmap.shape[0]\n",
    "        assert n_layers == heatmap.shape[1]\n",
    "        subplot_titles = np.empty((n_layers, n_layers), dtype=object)\n",
    "        # the following garbage is necessary because plotly indexes subplots\n",
    "        # starting from origin at top-left, but plotting heatmaps uses bottom-left origin.\n",
    "        # all transpose-related machinations are to account for this discrepancy\n",
    "        # smh\n",
    "        subplot_titles.fill(\"\")\n",
    "        for i in range(n_layers):\n",
    "            for j in range(n_layers):\n",
    "                if all_to_all or i < j:\n",
    "                    subplot_titles[i, n_layers - j - 1] = subplot_title_func(heatmap, i, j)\n",
    "        subplot_titles = subplot_titles.flatten(order=\"F\").tolist()\n",
    "\n",
    "    else:\n",
    "        assert heatmap.ndim == 2\n",
    "        assert all_to_all\n",
    "        IS_4D = False\n",
    "        n_layers = 1\n",
    "        heatmap = np.expand_dims(heatmap, axis=(0, 1))\n",
    "        subplot_titles = [\"\"]\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=n_layers, #max(n_layers-1, 1),\n",
    "        cols=n_layers,\n",
    "        subplot_titles=subplot_titles,\n",
    "    )\n",
    "\n",
    "    for from_i in range(n_layers):\n",
    "        for to_j in range(n_layers):\n",
    "            if not all_to_all and from_i >= to_j:\n",
    "                continue\n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=heatmap[from_i, to_j].T,\n",
    "                    x=np.arange(heatmap.shape[2]),\n",
    "                    y=np.arange(heatmap.shape[3]),\n",
    "                    colorscale=\"Viridis\",\n",
    "                    showscale=False,\n",
    "                    # name=subplot_title_func(heatmap, from_i, to_j) if subplot_title_func else \"\",\n",
    "                    zmin=cmin,\n",
    "                    zmax=cmax,\n",
    "                    # hovertext=h_argmax[i, j],\n",
    "                    texttemplate=\"%{z:.2f}\" if showtext else None,\n",
    "                    hovertemplate=heatmap_x_label+\": %{x}<br>\"+\n",
    "                    heatmap_y_label+\": %{y}<br>\"+\n",
    "                    heatmap_z_label+\": %{z:.3f}<br>\"\n",
    "                    # \"Component: %{text}<extra></extra>\",\n",
    "                    # colorbar=dict(\n",
    "                    #     title=\"Composition Score\",\n",
    "                    #     # titleside=\"right\",\n",
    "                    #     len=0.5,\n",
    "                    #     thickness=10,\n",
    "                    #     x=1.05,\n",
    "                    #     y=0.5,\n",
    "                    # ),\n",
    "                ),\n",
    "                row=n_layers - to_j,\n",
    "                col=from_i + 1,\n",
    "            )\n",
    "    fig.update_layout(**layout_dict)\n",
    "    # , xaxis_title=\"OV Head\", yaxis_title=\"QK Head\")\n",
    "    fig.data[-1].update(colorbar=dict(x=-0.05 if IS_4D else -0.1, y=0.5, thickness=20), showscale=True)\n",
    "    return fig\n",
    "\n",
    "def normalize_heatmap(heatmap) -> np.ndarray:\n",
    "    \"\"\"Normalize heatmap along the last axis (components) to sum to 1.\"\"\"\n",
    "    h = heatmap / (np.sum(heatmap, axis=-1, keepdims=True) + 1e-10)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e6e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "layer_weights_path_moeut = (\n",
    "    \"analysis_out/dump_slimpajama_moeut_small_matched_rope_noln_long/layer_weights.pth\"\n",
    ")\n",
    "layer_weights_path_baseline = (\n",
    "    \"analysis_out/dump_slimpajama_baseline_small_rope_long_nodrop_3/layer_weights.pth\"\n",
    ")\n",
    "layer_weights_path_moeut_g16 = \"analysis_out/dump_slimpajama_moeut_small_g16/layer_weights.pth\"\n",
    "layer_weights_path_baseline_20heads = (\n",
    "    \"analysis_out/dump_slimpajama_baseline_small_20heads/layer_weights.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1d08b",
   "metadata": {},
   "source": [
    "### Choose your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_weights_path = layer_weights_path_baseline_20heads\n",
    "ALL_TO_ALL = False\n",
    "layer_weights, heatmap = get_weights_and_heatmap_from_path(\n",
    "    layer_weights_path, all_to_all=ALL_TO_ALL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f42db7",
   "metadata": {},
   "source": [
    "## Get statistics for random matrices of the same shape as the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac612e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randoms\n",
    "d_embed = layer_weights[0][\"d_embed\"]\n",
    "d_head = layer_weights[0][\"d_head\"]\n",
    "random_heatmap = random_composition_heatmap(d_embed, d_head, n_layers=1, n_ov=4, n_qk=4)\n",
    "mean_max = np.mean(random_heatmap.max(axis=-1))\n",
    "mean_mean = np.mean(random_heatmap.mean(axis=-1))\n",
    "print(f\"Mean of max composition score: {mean_max} ± {np.std(random_heatmap.max(axis=-1))}\")\n",
    "print(f\"Mean of mean composition score: {mean_mean} ± {np.std(random_heatmap.mean(axis=-1))}\")\n",
    "\n",
    "# Random entropy:\n",
    "norm_random_heatmap = normalize_heatmap(random_heatmap)\n",
    "h_entropy_random = compute_entropy(norm_random_heatmap, axis=-1)\n",
    "print(f\"Random heatmap entropy: {h_entropy_random.mean()} ± {h_entropy_random.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f71a1",
   "metadata": {},
   "source": [
    "## Entropy analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6bba85",
   "metadata": {},
   "source": [
    "### Basic entropy over composition scores\n",
    "\n",
    "For x.OV talking to y.QK, what is the entropy of the composition scores across all d components?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7191f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_heatmap = normalize_heatmap(heatmap)\n",
    "h_entropy = compute_entropy(norm_heatmap, axis=-1)\n",
    "heatmap_entropy_grid_fig = plot_nd_heatmap_grid(\n",
    "    h_entropy,\n",
    "    all_to_all=ALL_TO_ALL,\n",
    "    cmin=2,\n",
    "    cmax=5, #h_entropy.max(),\n",
    "    layout_dict=dict(\n",
    "        title=\"Composition Scores Heatmap Grid (entropy over component comp scores)\",\n",
    "        autosize=False,\n",
    "        width=2000,\n",
    "        height=1500,\n",
    "    ),\n",
    ")\n",
    "# _save_path = Path(layer_weights_path).with_name(\"heatmap_entropy.png\")\n",
    "# heatmap_entropy_grid_fig.write_image(_save_path, \"png\", scale=2, width=2000, height=1500)\n",
    "heatmap_entropy_grid_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712f863",
   "metadata": {},
   "source": [
    "### OV subspace specificity\n",
    "\n",
    "We would like the same OV to write to the same subspace no matter which head it talks to.\n",
    "This can be captured by averaging the composition scores for a given OV across all QK heads which it talks to,\n",
    "and taking the entropy across this. It should be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ov_entropy = compute_entropy(norm_heatmap.mean(axis=(3,), keepdims=True), axis=-1) # average over QK heads\n",
    "\n",
    "avg_ov_entropy_fig = plot_nd_heatmap_grid(\n",
    "    avg_ov_entropy,\n",
    "    all_to_all=ALL_TO_ALL,\n",
    "    cmin=2,\n",
    "    cmax=5,\n",
    "    layout_dict=dict(\n",
    "        title=\"Average entropy of component weightings for OV heads (avg over QK heads) (↓=OV more specific)\",\n",
    "        autosize=False,\n",
    "        width=2000,\n",
    "        height=1600,\n",
    "        font=dict(size=12),  # Reduce overall font size including subplot titles\n",
    "    ),\n",
    "    showtext=False\n",
    ")\n",
    "# _save_path = Path(layer_weights_path).with_name(\"avg_ov_entropy.jpg\")\n",
    "# avg_ov_entropy_fig.write_image(_save_path, \"jpg\", scale=1)\n",
    "avg_ov_entropy_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb387e8",
   "metadata": {},
   "source": [
    "## QK subspace specificity\n",
    "\n",
    "To what extent does one QK read from the same subspace regardless of which OV it's talking to?\n",
    "This can be captured by averaging composition scores across all OVs for a given QK, and taking the entropy of the resulting\n",
    "distribution.\n",
    "\n",
    "I don't know whether we want this to be low (QK only reads from the same subspace) or high\n",
    "(QK can perform its computation across many different properties???)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b27380",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_qk_entropy = compute_entropy(norm_heatmap.mean(axis=(2,), keepdims=True), axis=-1) # average over OV heads\n",
    "\n",
    "avg_qk_entropy_fig = plot_nd_heatmap_grid(\n",
    "    avg_qk_entropy,\n",
    "    all_to_all=ALL_TO_ALL,\n",
    "    cmin=2,\n",
    "    cmax=5,\n",
    "    layout_dict=dict(\n",
    "        title=\"Average Entropy of Component Weightings Between Different QK Heads (avg over OV heads) (?)\",\n",
    "        autosize=False,\n",
    "        width=2000,\n",
    "        height=1600,\n",
    "        font=dict(size=12),  # Reduce overall font size including subplot titles\n",
    "    ),\n",
    "    showtext=False\n",
    ")\n",
    "# _save_path = Path(layer_weights_path).with_name(\"avg_qk_entropy.jpg\")\n",
    "# avg_qk_entropy_fig.write_image(_save_path, \"jpg\", scale=1)\n",
    "avg_qk_entropy_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791bea8",
   "metadata": {},
   "source": [
    "### Overall entropy\n",
    "Finally, we want on average that different OV-QK pairings communicate across different channels. The more this is the case,\n",
    "the more distinct properties can be stored in the representational space.\n",
    "\n",
    "This can be captured by averaging composition scores over all OV-QK pairs, and taking the entropy.\n",
    "\n",
    "We want this to be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6739ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_layer_entropy = compute_entropy(norm_heatmap.mean(axis=(2, 3)), axis=-1) # average over OV and QK heads\n",
    "\n",
    "avg_layer_entropy_fig = plot_nd_heatmap_grid(\n",
    "    avg_layer_entropy,\n",
    "    all_to_all=True,\n",
    "    cmin=2,\n",
    "    cmax=5,\n",
    "    layout_dict=dict(\n",
    "        title=\"Average Entropy of Component Weightings Between All OV-QK Pairs in Each Layer Pair (↑)\",\n",
    "        autosize=False,\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "        font=dict(size=12),  # Reduce overall font size including subplot titles\n",
    "    ),\n",
    "    showtext=True,\n",
    "    heatmap_x_label=\"Layer From\",\n",
    "    heatmap_y_label=\"Layer To\",\n",
    "    heatmap_z_label=\"Entropy\"\n",
    ")\n",
    "# _save_path = Path(layer_weights_path).with_name(\"avg_layer_entropy.jpg\")\n",
    "# avg_layer_entropy_fig.write_image(_save_path, \"jpg\", scale=1)\n",
    "avg_layer_entropy_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b6af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
